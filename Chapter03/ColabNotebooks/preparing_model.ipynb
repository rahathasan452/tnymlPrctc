{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up_l5PTC9wax"
      },
      "source": [
        "# Chapter 3 - Building a weather station with TensorFlow Lite for Microcontrollers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3sj_09sGTtQ",
        "outputId": "f56a43ba-05e8-48d9-ba7b-de7524585ee6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLIttRZCUJKA"
      },
      "source": [
        "### Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7D1EPWeUUNPa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn.metrics\n",
        "import tensorflow as tf\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZINNrGD9RECB"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6RJIJgR6Q7t-"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "MELTING_TEMPERATURE = 2\n",
        "MIN_SNOW_CM = 0.5 # Above this value, we consider it as snow\n",
        "NUM_EPOCHS = 20\n",
        "OUTPUT_DATASET_FILE = \"snow_dataset.csv\"\n",
        "TFL_MODEL_FILE = \"snow_forecast_model.tflite\"\n",
        "TFL_MODEL_HEADER_FILE = \"snow_forecast_model.h\"\n",
        "TF_MODEL = \"snow_forecast\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mrP29FLiiKe"
      },
      "source": [
        "**Importing weather data from WorldWeatherOnline**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## if wwo dosnt work use open meteo for data scraping"
      ],
      "metadata": {
        "id": "KsZADcBGhZOZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLmg4EGROrX"
      },
      "source": [
        "### Install the www-hist package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSY57BiSLP78"
      },
      "outputs": [],
      "source": [
        "!pip install wwo-hist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wwo-hist --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vJmFUW2LDVB",
        "outputId": "6ca20f05-92e2-4802-98fe-24cae10b34ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wwo-hist in /usr/local/lib/python3.10/dist-packages (0.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnKh8e2MS8sb"
      },
      "source": [
        "### Import retrieve_hist_data function from wwo-hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6w-_xrp8S_49"
      },
      "outputs": [],
      "source": [
        "from wwo_hist import retrieve_hist_data # WorldWeatherOnline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuEPsg_XTDas"
      },
      "source": [
        "### Acquire data for ten years (01-JAN-2011 to 31-DEC-2020) with an hourly frequency from Canazei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOYKjRGJRm7D",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "frequency=1\n",
        "api_key = 'ebb1061d03b04860a83213432242811'\n",
        "location_list = ['canazei']\n",
        "\n",
        "# retrieve_hist_data returns a list of dataframe(s)\n",
        "df_weather = retrieve_hist_data(api_key,\n",
        "                                location_list,\n",
        "                                '01-JAN-2011',\n",
        "                                '31-DEC-2020',\n",
        "                                frequency,\n",
        "                                location_label = False,\n",
        "                                export_csv = False,\n",
        "                                store_df = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fetch data from open-Meteo"
      ],
      "metadata": {
        "id": "-Sg5ckXJU5fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests pandas\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uhIGNIfZU4i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def fetch_weather_data(start_date, end_date, latitude, longitude):\n",
        "    # Initialize an empty DataFrame\n",
        "    all_data = pd.DataFrame()\n",
        "\n",
        "    # Convert string dates to datetime objects\n",
        "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "    # Loop through each day (Open-Meteo supports daily queries for historical data)\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        # Open-Meteo API URL for historical data\n",
        "        url = (\n",
        "            f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
        "            f\"latitude={latitude}&longitude={longitude}&start_date={date_str}\"\n",
        "            f\"&end_date={date_str}&hourly=temperature_2m,relative_humidity_2m,snowfall\"\n",
        "        )\n",
        "\n",
        "        # Fetch data from the API\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if 'hourly' in data:\n",
        "                # Create a DataFrame for hourly data\n",
        "                df = pd.DataFrame(data['hourly'])\n",
        "                df['datetime'] = pd.to_datetime(data['hourly']['time'])\n",
        "                all_data = pd.concat([all_data, df], ignore_index=True)\n",
        "        else:\n",
        "            print(f\"Failed to fetch data for {date_str}: {response.status_code}\")\n",
        "\n",
        "        # Increment the date by one day\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "    # Reset index and return the DataFrame\n",
        "    return all_data.reset_index(drop=True)\n",
        "\n",
        "# Fetch data for Canazei (latitude: 46.4775, longitude: 11.7711)\n",
        "latitude = 46.4775\n",
        "longitude = 11.7711\n",
        "start_date = \"2013-12-15\"\n",
        "end_date = \"2014-4-28\"\n",
        "\n",
        "# Call the function to fetch data\n",
        "weather_data = fetch_weather_data(start_date, end_date, latitude, longitude)\n",
        "\n",
        "# Save or display the data\n",
        "weather_data.to_csv(\"canazei_weather_data.csv\", index=False)\n",
        "print(weather_data.head())\n"
      ],
      "metadata": {
        "id": "w8rQodV3VC5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxlPRAvFRGjf"
      },
      "source": [
        "### Export temperature, humidity, and output snowfall to lists\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwPPANExDPeA"
      },
      "outputs": [],
      "source": [
        "# Extract temperature, humidity and precipitation\n",
        "t_list = df_weather[0].tempC.astype(float).to_list()\n",
        "h_list = df_weather[0].humidity.astype(float).to_list()\n",
        "s_list = df_weather[0].totalSnow_cm.astype(float).to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZridbJUuR-jt"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9t51laOtkA3"
      },
      "source": [
        "### Explore the extracted physical quantities in a 2D scatter chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wImIp3R9tnW0"
      },
      "outputs": [],
      "source": [
        "def binarize(snow, threshold):\n",
        "  if snow > threshold:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "s_bin_list = [binarize(snow, MIN_SNOW_CM) for snow in s_list]\n",
        "\n",
        "cm = plt.cm.get_cmap('gray_r')\n",
        "plt.figure(dpi=150)\n",
        "sc = plt.scatter(t_list, h_list, c=s_bin_list, cmap=cm, label=\"Snow\")\n",
        "plt.colorbar(sc)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title(\"Snow(T, H)\")\n",
        "plt.xlabel(\"Temperature - °C\")\n",
        "plt.ylabel(\"Humidity - %\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boDIPMWTSdPZ"
      },
      "source": [
        "### Generate the output labels (Yes and No)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xe6nbGpSif2"
      },
      "outputs": [],
      "source": [
        "def gen_label(snow, temperature):\n",
        "  if snow > MIN_SNOW_CM and temperature < MELTING_TEMPERATURE:\n",
        "    return \"Yes\"\n",
        "  else:\n",
        "    return \"No\"\n",
        "\n",
        "snow_labels = [gen_label(snow, temp) for snow, temp in zip(s_list, t_list)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EbC3q1qTQuJ"
      },
      "source": [
        "### Build the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV7hQfqda6Y2"
      },
      "outputs": [],
      "source": [
        "csv_header = [\"Temp0\", \"Temp1\", \"Temp2\", \"Humi0\", \"Humi1\", \"Humi2\", \"Snow\"]\n",
        "\n",
        "df_dataset = pd.DataFrame(list(zip(t_list[:-2], t_list[1:-1], t_list[2:], h_list[:-2], h_list[1:-1], h_list[2:], snow_labels[2:])), columns = csv_header)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTYLr7fg_4Vm"
      },
      "source": [
        "### Balance the dataset by undersampling the majority class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2hxpmrLINEf"
      },
      "outputs": [],
      "source": [
        "df0 = df_dataset[df_dataset['Snow'] == \"No\"]\n",
        "df1 = df_dataset[df_dataset['Snow'] == \"Yes\"]\n",
        "\n",
        "num_nosnow_samples_old = round((len(df0.index) / (len(df_dataset.index))) * 100, 2)\n",
        "num_snow_samples_old   = round((len(df1.index) / (len(df_dataset.index))) * 100, 2)\n",
        "\n",
        "# Random subsampling of the majority class to guarantee 50% split\n",
        "if len(df1.index) < len(df0.index):\n",
        "  df0_sub = df0.sample(len(df1.index))\n",
        "  df_dataset = pd.concat([df0_sub, df1])\n",
        "else:\n",
        "  df1_sub = df1.sample(len(df0.index))\n",
        "  df_dataset = pd.concat([df1_sub, df0])\n",
        "\n",
        "df0 = df_dataset[df_dataset['Snow'] == \"No\"]\n",
        "df1 = df_dataset[df_dataset['Snow'] == \"Yes\"]\n",
        "\n",
        "num_nosnow_samples_new = round((len(df0.index) / (len(df_dataset.index))) * 100, 2)\n",
        "num_snow_samples_new = round((len(df1.index) / (len(df_dataset.index))) * 100, 2)\n",
        "\n",
        "# Show number of samples\n",
        "df_samples_results = pd.DataFrame.from_records(\n",
        "                [[\"% No Snow\", num_nosnow_samples_old, num_nosnow_samples_new],\n",
        "                [\"% Snow\", num_snow_samples_old, num_snow_samples_new]],\n",
        "            columns = [\"Class\", \"Before - %\", \"After - %\"], index=\"Class\").round(2)\n",
        "\n",
        "display(df_samples_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiFURGcVRkZ9"
      },
      "source": [
        "### Scale the input features with Z-score independently\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuEhj6eEJyqP"
      },
      "outputs": [],
      "source": [
        "# Get all values\n",
        "t_list = df_dataset['Temp0'].tolist()\n",
        "h_list = df_dataset['Humi0'].tolist()\n",
        "t_list = t_list + df_dataset['Temp2'].tail(2).tolist()\n",
        "h_list = h_list + df_dataset['Humi2'].tail(2).tolist()\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "t_avg = mean(t_list)\n",
        "h_avg = mean(h_list)\n",
        "t_std = std(t_list)\n",
        "h_std = std(h_list)\n",
        "print(\"COPY ME!\")\n",
        "print(\"Temperature - [MEAN, STD]  \", round(t_avg, 5), round(t_std, 5))\n",
        "print(\"Humidity - [MEAN, STD]     \", round(h_avg, 5), round(h_std, 5))\n",
        "\n",
        "# Scaling with Z-score function\n",
        "def scaling(val, avg, std):\n",
        "  return (val - avg) / (std)\n",
        "\n",
        "df_dataset['Temp0'] = df_dataset['Temp0'].apply(lambda x: scaling(x, t_avg, t_std))\n",
        "df_dataset['Temp1'] = df_dataset['Temp1'].apply(lambda x: scaling(x, t_avg, t_std))\n",
        "df_dataset['Temp2'] = df_dataset['Temp2'].apply(lambda x: scaling(x, t_avg, t_std))\n",
        "df_dataset['Humi0'] = df_dataset['Humi0'].apply(lambda x: scaling(x, h_avg, h_std))\n",
        "df_dataset['Humi1'] = df_dataset['Humi1'].apply(lambda x: scaling(x, h_avg, h_std))\n",
        "df_dataset['Humi2'] = df_dataset['Humi2'].apply(lambda x: scaling(x, h_avg, h_std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psfnQU5ySl8P"
      },
      "source": [
        "### Visualize raw/scaled input features distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8HeV9JhSdM8"
      },
      "outputs": [],
      "source": [
        "t_norm_list = df_dataset['Temp0'].tolist()\n",
        "h_norm_list = df_dataset['Humi0'].tolist()\n",
        "t_norm_list = t_norm_list + df_dataset['Temp2'].tail(2).tolist()\n",
        "h_norm_list = h_norm_list + df_dataset['Humi2'].tail(2).tolist()\n",
        "\n",
        "fig, ax=plt.subplots(1,2)\n",
        "plt.subplots_adjust(wspace = 0.4)\n",
        "sns.distplot(t_list, ax=ax[0])\n",
        "ax[0].set_title(\"Un-normalized temperature\")\n",
        "sns.distplot(h_list, ax=ax[1])\n",
        "ax[1].set_title(\"Un-normalized humidity\")\n",
        "\n",
        "fig, ax=plt.subplots(1,2)\n",
        "plt.subplots_adjust(wspace = 0.5)\n",
        "sns.distplot(t_norm_list, ax=ax[0])\n",
        "ax[0].set_title(\"Normalized temperature\")\n",
        "sns.distplot(h_norm_list, ax=ax[1])\n",
        "ax[1].set_title(\"Normalized humidity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdYHA2tsWRue"
      },
      "source": [
        "### Export to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8VMeyS3WVTE"
      },
      "outputs": [],
      "source": [
        "df_dataset.to_csv(OUTPUT_DATASET_FILE, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvOQA_91OTXi"
      },
      "source": [
        "## Training the ML model with TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnZm1IWfO0R5"
      },
      "source": [
        "### Extract the input features and output labels from the df_dataset Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdgh78PbO88o"
      },
      "outputs": [],
      "source": [
        "f_names = df_dataset.columns.values[0:6]\n",
        "l_name  = df_dataset.columns.values[6:7]\n",
        "x = df_dataset[f_names]\n",
        "y = df_dataset[l_name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5cHGmqmPB8j"
      },
      "source": [
        "### Encode the labels to numerical values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRj3fc4EPJOi"
      },
      "outputs": [],
      "source": [
        "labelencoder = LabelEncoder()\n",
        "labelencoder.fit(y.Snow)\n",
        "y_encoded = labelencoder.transform(y.Snow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4esjhR8nPOQv"
      },
      "source": [
        "### Split the dataset into train, validation, and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNBv7bSQPTyi"
      },
      "outputs": [],
      "source": [
        "# Split 1 (85% vs 15%)\n",
        "x_train, x_validate_test, y_train, y_validate_test = train_test_split(x, y_encoded, test_size=0.15, random_state = 1)\n",
        "# Split 2 (50% vs 50%)\n",
        "x_test, x_validate, y_test, y_validate = train_test_split(x_validate_test, y_validate_test, test_size=0.50, random_state = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvT6XrP5Pawx"
      },
      "source": [
        "### Create the model with Keras API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RsMj7kKPgAo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(layers.Dense(12, activation='relu', input_shape=(len(f_names),)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAQWbyvcPlET"
      },
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI3vhc6IPpLb"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp_91rLWPt6Y"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7BLJQrlPxWS"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_validate, y_validate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_9CWAsDQBsM"
      },
      "source": [
        "### Analyze the accuracy and loss after each training epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-cKFnAEQJYm"
      },
      "outputs": [],
      "source": [
        "loss_train = history.history['loss']\n",
        "loss_val   = history.history['val_loss']\n",
        "acc_train  = history.history['accuracy']\n",
        "acc_val    = history.history['val_accuracy']\n",
        "epochs     = range(1, NUM_EPOCHS + 1)\n",
        "\n",
        "def plot_train_val_history(x, y_train, y_val, type_txt):\n",
        "  plt.figure(figsize = (10,7))\n",
        "  plt.plot(x, y_train, 'g', label='Training'+type_txt)\n",
        "  plt.plot(x, y_val, 'b', label='Validation'+type_txt)\n",
        "  plt.title('Training and Validation'+type_txt)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(type_txt)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_train_val_history(epochs, loss_train, loss_val, \"Loss\")\n",
        "plot_train_val_history(epochs, acc_train, acc_val, \"Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH5k1yxIQX9l"
      },
      "source": [
        "### Save the entire TensorFlow model as a SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkfSnfTaQcaA"
      },
      "outputs": [],
      "source": [
        "model.save(TF_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssiWwrM2TO3b"
      },
      "source": [
        "## Evaluating the model effectiveness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBkNtdfITaLP"
      },
      "source": [
        "### Visualize the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnXH1AK-TV6m"
      },
      "outputs": [],
      "source": [
        "y_test_pred = model.predict(x_test)\n",
        "\n",
        "y_test_pred = (y_test_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "cm = sklearn.metrics.confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "index_names  = [\"Actual No Snow\", \"Actual Snow\"]\n",
        "column_names = [\"Predicted No Snow\", \"Predicted Snow\"]\n",
        "\n",
        "df_cm = pd.DataFrame(cm, index = index_names, columns = column_names)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='d', cmap=\"Blues\")\n",
        "plt.figure(figsize = (10,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVo3KjpNTmMM"
      },
      "source": [
        "### Calculate Recall, Precision, and F-score performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KewMzVMTsdm"
      },
      "outputs": [],
      "source": [
        "TN = cm[0][0]\n",
        "TP = cm[1][1]\n",
        "FN = cm[1][0]\n",
        "FP = cm[0][1]\n",
        "\n",
        "accuracy = (TP + TN) / (TP + TN + FN + FP)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f_score = (2 * recall * precision) / (recall + precision)\n",
        "\n",
        "print(\"Accuracy:  \", round(accuracy, 3))\n",
        "print(\"Recall:    \", round(recall, 3))\n",
        "print(\"Precision: \", round(precision, 3))\n",
        "print(\"F-score:   \", round(f_score, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G3USDyST8XC"
      },
      "source": [
        "## Quantizing the model with TFLite converter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwVXYHLrUBpW"
      },
      "source": [
        "### Select a few hundred of samples randomly from the test dataset to calibrate the quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1z6BqPZUS-5"
      },
      "outputs": [],
      "source": [
        "def representative_data_gen():\n",
        "  for i_value in tf.data.Dataset.from_tensor_slices(x_test).batch(1).take(100):\n",
        "    i_value_f32 = tf.dtypes.cast(i_value, tf.float32)\n",
        "    yield [i_value_f32]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHItQuK8UW21"
      },
      "source": [
        "### Import the TensorFlow SavedModel directory into TensorFlow Lite Converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLzuXGy_UbbA"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(TF_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um7vdQI3Ur2P"
      },
      "source": [
        "### Initialize TensorFlow Lite converter for the 8-bit quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qs7BI6HUwVu"
      },
      "outputs": [],
      "source": [
        "converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa9y3-hGU0H2"
      },
      "source": [
        "### Convert the model to TensorFlow Lite file format (FlatBuffers) as save it as .tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6vhVG09U82z"
      },
      "outputs": [],
      "source": [
        "tflite_model_quant = converter.convert()\n",
        "open(TFL_MODEL_FILE, \"wb\").write(tflite_model_quant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMYG7uZHVM2_"
      },
      "source": [
        "### Convert the TensorFlow Lite model to C-byte array with xxd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzE0vsBfVS1p"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get -qq install xxd\n",
        "!xxd -i snow_forecast_model.tflite > model.h\n",
        "!cat model.h"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter03.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}